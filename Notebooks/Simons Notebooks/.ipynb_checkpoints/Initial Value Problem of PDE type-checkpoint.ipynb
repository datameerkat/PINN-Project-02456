{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903b8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from timeit import default_timer as tdt\n",
    "\n",
    "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    my_device = 'cuda'\n",
    "else:\n",
    "    my_device = 'cpu'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52026f77",
   "metadata": {},
   "source": [
    "# ============ Setup ============\n",
    "We define the exact solution as: u(x,t) = h(x - c*t) and h(x, t=0) = exp(-alpha * x^2)\n",
    "\n",
    "PDE: du/dt = -c * du/dx\n",
    "BC:  u(x_a, t) = u(x_b, t) where x_a is the leftmost boundary and x_b is the rightmost boundary\n",
    "IC:  u(x, t=0) = exp(-alpha * x^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8c8e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining h which is the initial wave shape\n",
    "def h(x, alpha=3.0):\n",
    "    return torch.exp(-alpha * x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "723f1aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the initial wave shape\n",
    "x_lin = torch.linspace(0, 10.0, 500)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "ax.plot(x_lin, h(x_lin), linestyle='-', color='k')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('u(x,t=0)')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faeb9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers_size, output_size):\n",
    "        super(FFN, self).__init__()\n",
    "        \n",
    "        n_layers = len(hidden_layers_size)\n",
    "        HL = hidden_layers_size\n",
    "        \n",
    "        # Define layers\n",
    "        self.L1 = nn.Linear(input_size, HL[0], bias=True)\n",
    "        self.L2 = nn.Linear(HL[0], HL[1], bias=True)\n",
    "        \n",
    "        self.output = nn.Linear(HL[-1], output_size, bias=True)\n",
    "        \n",
    "        # Define activation function\n",
    "        self.act_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layers\n",
    "        x = self.act_fn( self.L1(x) )\n",
    "        x = self.act_fn( self.L2(x) )\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define network\n",
    "net = FFN(2, [32, 32], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76ffc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss = torch.nn.MSELoss()\n",
    "\n",
    "def get_loss_PDE(x, t, net, c):\n",
    "    # \n",
    "    xt = torch.cat([x, t], dim=1)\n",
    "    u_net = net(xt)\n",
    "    \n",
    "    # Calculate du/dx\n",
    "    dudx = torch.autograd.grad(u_net, x, grad_outputs=torch.ones_like(u_net), create_graph=True)[0]\n",
    "    # Calculate du/dt\n",
    "    dudt = torch.autograd.grad(u_net, t, grad_outputs=torch.ones_like(u_net), create_graph=True)[0]\n",
    "    \n",
    "    # Setup residuals\n",
    "    lhs = dudt\n",
    "    rhs = -c * dudx\n",
    "    \n",
    "    loss = MSE_loss(lhs, rhs)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_loss_BC(x_bound, t_bound, net):\n",
    "    x_a = torch.tensor( x_bound[0] ).repeat(t_bound.shape).requires_grad_(True)\n",
    "    x_b = torch.tensor( x_bound[1] ).repeat(t_bound.shape).requires_grad_(True)\n",
    "    \n",
    "    # Create vector of points on leftmost boundary at all times t_bound\n",
    "    xa_t = torch.cat([x_a, t_bound], dim=1)\n",
    "    \n",
    "    # Create vector of points on leftmost boundary at all times t_bound\n",
    "    xb_t = torch.cat([x_b, t_bound], dim=1)\n",
    "    \n",
    "    # Calculate the net result on the left and the right boundary\n",
    "    u_a = net(xa_t)\n",
    "    u_b = net(xb_t)\n",
    "    \n",
    "    # Periodic boundary so the result at all xa and xb at all times should be the same.\n",
    "    loss = MSE_loss(u_a, u_b)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_loss_IC(x, t0, net):\n",
    "    # Get net result at initial state\n",
    "    t0_repeated = t0.repeat(x.shape)\n",
    "    xt = torch.cat([x, t0_repeated], dim=1)\n",
    "    u_net = net(xt)\n",
    "    \n",
    "    # Get true initial state\n",
    "    u_init = h(x)\n",
    "    \n",
    "    loss = MSE_loss(u_net, u_init)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97988422",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m t_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(t0[\u001b[38;5;241m0\u001b[39m], t_end[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m500\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Loss PDE\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m test_loss_PDE \u001b[38;5;241m=\u001b[39m get_loss_PDE(x_test, t_test, net, c\u001b[38;5;241m=\u001b[39mc)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss PDE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_PDE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Loss BC\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 6\u001b[0m, in \u001b[0;36mget_loss_PDE\u001b[1;34m(x, t, net, c)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loss_PDE\u001b[39m(x, t, net, c):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m      5\u001b[0m     xt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     u_net \u001b[38;5;241m=\u001b[39m net(xt)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Calculate du/dx\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     dudx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u_net, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u_net), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning_1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[29], line 20\u001b[0m, in \u001b[0;36mFFN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Hidden layers\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL1(x) )\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL2(x) )\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Output layer\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning_1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning_1\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# Define wavespeed\n",
    "c = 0.2\n",
    "\n",
    "# Setup the domain\n",
    "x_bound = (0.0, 10.0)\n",
    "t_span = (0.0, 25.0)\n",
    "\n",
    "\n",
    "######  Check that loss functions function\n",
    "x_test = torch.linspace(x_bound[0], x_bound[1], 500, requires_grad=True).view(-1,1)\n",
    "t_test = torch.linspace(t0[0], t_end[0], 500, requires_grad=True).view(-1,1)\n",
    "# Loss PDE\n",
    "test_loss_PDE = get_loss_PDE(x_test, t_test, net, c=c)\n",
    "print(f'Test loss PDE: {test_loss_PDE}')\n",
    "# Loss BC\n",
    "test_loss_BC = get_loss_BC(x_bound, t_test, net)\n",
    "print(f'Test loss BC : {test_loss_BC}')\n",
    "# Loss IC\n",
    "test_loss_IC = get_loss_IC(x_test, t_span[0], net)\n",
    "print(f'Test loss IC : {test_loss_IC}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cc3b950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon Eberhard\\AppData\\Local\\Temp\\ipykernel_16428\\4179842110.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_a = torch.tensor( x_bound[0] ).repeat(t_bound.shape).requires_grad_(True)\n",
      "C:\\Users\\Simon Eberhard\\AppData\\Local\\Temp\\ipykernel_16428\\4179842110.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_b = torch.tensor( x_bound[1] ).repeat(t_bound.shape).requires_grad_(True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epochs_to_make_change' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m loss3\u001b[38;5;241m.\u001b[39mappend( loss_from_IC )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# SoftAdapt\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m epochs_to_make_change \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     43\u001b[0m     adapt_weights \u001b[38;5;241m=\u001b[39m softadapt_obj\u001b[38;5;241m.\u001b[39mget_component_weights(torch\u001b[38;5;241m.\u001b[39mtensor(loss1),\n\u001b[0;32m     44\u001b[0m                                                         torch\u001b[38;5;241m.\u001b[39mtensor(loss2),\n\u001b[0;32m     45\u001b[0m                                                         torch\u001b[38;5;241m.\u001b[39mtensor(loss3),\n\u001b[0;32m     46\u001b[0m                                                         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m     loss1, loss2, loss3 \u001b[38;5;241m=\u001b[39m [], [], []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs_to_make_change' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Setup optimizer\n",
    "learning_rate = 3e-4\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 20\n",
    "save_every_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "loss1, loss2, loss3 = [], [], []\n",
    "net = net.to(my_device)\n",
    "x_bound_on_device = torch.FloatTensor(x_bound).to(my_device)\n",
    "t_span_on_device = torch.FloatTensor(t_span).to(my_device)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sample t\n",
    "    t_points = torch.rand(batch_size, 1, device=my_device, requires_grad=True) * t_span_on_device[1]\n",
    "    # Fixed point like Karpathy\n",
    "    #t_points = torch.linspace(0, 10, batch_size, device=my_device, requires_grad=True).view(-1,1)\n",
    "    \n",
    "    # Sample x\n",
    "    x_points = torch.rand(batch_size, 1, device=my_device, requires_grad=True) * x_bound_on_device[1]\n",
    "    \n",
    "    ######## Calculate loss\n",
    "    # Loss from ODE\n",
    "    loss_from_PDE = get_loss_PDE(x_points, t_points, net, c=c)\n",
    "    # Loss from BC\n",
    "    loss_from_BC = get_loss_BC(x_bound_on_device, t_points, net)\n",
    "    # Loss from IC\n",
    "    loss_from_IC = get_loss_IC(x_points, t_span_on_device[0], net)\n",
    "    \n",
    "    loss1.append( loss_from_PDE )\n",
    "    loss2.append( loss_from_BC )\n",
    "    loss3.append( loss_from_IC )\n",
    "\n",
    "    # SoftAdapt\n",
    "    if epoch % epochs_to_make_change == 0 and epoch != 0:\n",
    "        adapt_weights = softadapt_obj.get_component_weights(torch.tensor(loss1),\n",
    "                                                            torch.tensor(loss2),\n",
    "                                                            torch.tensor(loss3),\n",
    "                                                            verbose=False)\n",
    "        loss1, loss2, loss3 = [], [], []\n",
    "    \n",
    "    loss = adapt_weights[0] * loss_from_PDE + adapt_weights[1] * loss_from_BC + adapt_weights[2] * loss_from_IC\n",
    "    #loss = loss_from_ODE + loss_from_IC\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "          \n",
    "    if (epoch+1) % save_every_epochs == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:6}, Loss: {loss.item():8.5f}, ode_loss: {loss_from_PDE:8.7f}, bc_loss: {loss_from_BC:8.7f}\"\n",
    "              f\", ic_loss: {loss_from_BC:8.7f}\"\n",
    "              f\", aw[0]: {adapt_weights[0]:8.5f}, aw[1]: {adapt_weights[1]:8.5f}, aw[2]: {adapt_weights[2]:8.5f}\")\n",
    "        epochs_saved.append(epoch+1)\n",
    "        loss_saved.append(loss.item())\n",
    "        ode_res_saved.append(loss_from_ODE.detach().to('cpu'))\n",
    "        IC_loss_saved.append(loss_from_IC.detach().to('cpu'))\n",
    "        adapt_ode_loss_saved.append( adapt_weights[0]*loss_from_ODE.detach().to('cpu') )\n",
    "        adapt_IC_loss_saved.append( adapt_weights[1]*loss_from_IC.detach().to('cpu') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65137f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a896f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c300eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ba180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5e80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef45b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning_1] *",
   "language": "python",
   "name": "conda-env-deep_learning_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
