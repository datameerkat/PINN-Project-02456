{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b250a993",
   "metadata": {},
   "source": [
    "## PINN for predicting a SIR model\n",
    "This is the base case of modeling a SIR model using a Feed Forward Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9ebd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ac5fb-2d17-4360-aaf7-eefe9d754914",
   "metadata": {},
   "source": [
    "### Define time, initial conditions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ab05ff-211b-415c-88b5-d2cd39d2290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define amount of days being predicted and create time tensor\n",
    "t_days = 500\n",
    "t_tensor = torch.linspace(0, t_days, t_days+1, requires_grad = True).view(-1, 1)\n",
    "t_0 = torch.tensor([0], dtype=torch.float32).view(-1,1)\n",
    "\n",
    "#define initial conditions for SIR model\n",
    "S_0 = 9999\n",
    "I_0 = 1\n",
    "R_0 = 0\n",
    "N = S_0 + I_0 + R_0\n",
    "\n",
    "#define model parameters\n",
    "alpha = 0.05\n",
    "beta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d8634e-c78c-4a24-87f5-36a70c8ba636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR_numerical(t_array):    \n",
    "    # Initialize arrays of data\n",
    "    # S: Susceptibles, I: Infected/Infectious, R: Recovered/Removed \n",
    "    I_array = np.zeros(t_days + 1)\n",
    "    S_array = np.zeros(t_days + 1)\n",
    "    R_array = np.zeros(t_days + 1)\n",
    "    N_array = np.zeros(t_days + 1)\n",
    "\n",
    "    # Initialize values. Population is sucseptible, small amount infected and 0 recovered\n",
    "    # I: Infected is typically first\n",
    "    I_init = 1\n",
    "    S_init = N - I_init\n",
    "    R_init = 0\n",
    "\n",
    "    I_array[0] = I_init\n",
    "    S_array[0] = S_init\n",
    "    R_array[0] = R_init\n",
    "    N_array[0] = N\n",
    "\n",
    "    # Number of days between data points\n",
    "    dt = 1\n",
    "\n",
    "    for i in range(1, t_days + 1):\n",
    "        I = I_array[i-1]\n",
    "        S = S_array[i-1]\n",
    "        R = R_array[i-1]\n",
    "\n",
    "        dSdt = -alpha*(S*I)/N\n",
    "        dIdt = alpha*(S*I)/N - beta*I\n",
    "        dRdt = beta*I\n",
    "\n",
    "        S_array[i] = S + dt*dSdt\n",
    "        I_array[i] = I + dt*dIdt\n",
    "        R_array[i] = R + dt*dRdt\n",
    "        N_array[i] = S + I + R\n",
    "        \n",
    "    return S_array, I_array, R_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f18a558-3494-4e81-9490-407617c376ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute numerical solution for loss_obs\n",
    "t_array = np.arange(0, t_days + 1, 1)\n",
    "S_array, I_array, R_array = SIR_numerical(t_array)\n",
    "S_obs = torch.tensor(S_array, dtype=torch.float32, requires_grad=True).view(-1,1)\n",
    "I_obs = torch.tensor(I_array, dtype=torch.float32, requires_grad=True).view(-1,1)\n",
    "R_obs = torch.tensor(R_array, dtype=torch.float32, requires_grad=True).view(-1,1)\n",
    "SIR_obs = torch.cat((S_obs, I_obs, R_obs), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d14d397-5e6b-44fe-8e36-32fac5dc97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ode(net, t):\n",
    "    SIR = net(t)\n",
    "    \n",
    "    # Calculate derivates\n",
    "    dSdt = torch.autograd.grad(SIR[:,0], t, grad_outputs=SIR[:,0], create_graph=True)[0]\n",
    "    dIdt = torch.autograd.grad(SIR[:,1], t, grad_outputs=SIR[:,1], create_graph=True)[0]\n",
    "    dRdt = torch.autograd.grad(SIR[:,2], t, grad_outputs=SIR[:,2], create_graph=True)[0]\n",
    "    \n",
    "    # Calculate the residuals using vectorized operations\n",
    "    S, I, R = SIR[:, 0], SIR[:, 1], SIR[:, 2]\n",
    "    S_r = dSdt - (- beta * S * I)\n",
    "    I_r = dIdt - (beta * S * I - alpha * I)\n",
    "    R_r = dRdt - (alpha * I)\n",
    "    \n",
    "    # Combine the residuals into a single tensor and calculate the mean squared error\n",
    "    residuals = torch.cat((S_r, I_r, R_r))\n",
    "    loss_ode = torch.mean(residuals**2)\n",
    "    return loss_ode\n",
    "\n",
    "\n",
    "#loss function for initial conditoons of S, I and R\n",
    "def loss_ic(net):\n",
    "    SIR_t0 = net(t_0)\n",
    "    loss_ic_squared = (SIR_t0[:, 0]-S_0)**2 + (SIR_t0[:, 1]-I_0)**2 + (SIR_t0[:, 2]-R_0)**2\n",
    "    loss_ic = torch.sqrt(loss_ic_squared)/3\n",
    "    return loss_ic\n",
    "\n",
    "def loss_obs(net, t, SIR_obs):\n",
    "    SIR_net = net(t)\n",
    "    loss_obs = torch.nn.functional.mse_loss(SIR_net, SIR_obs)\n",
    "    return loss_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea83179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network architecture\n",
    "input_dim = 1\n",
    "output_dim = 3\n",
    "num_hidden = 20\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, output_dim),\n",
    "        )\n",
    "        \n",
    "        # Apply Kaiming initialization to the layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, t):\n",
    "        SIR = self.linear_relu_stack(t)\n",
    "        return SIR\n",
    "    \n",
    "net = Net(num_hidden)\n",
    "    \n",
    "# hyperparameters\n",
    "learning_rate = 1e-6\n",
    "batch_size = 20\n",
    "num_epochs = 1000\n",
    "\n",
    "#initialize lambdas for soft-adaptation\n",
    "lambda_ode = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "lambda_ic = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "lambda_obs = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "\n",
    "#optimizer: weights updates the net, ode and ic update the lambda for soft-adaptation\n",
    "optimizer_weights = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer_ode = optim.Adam([lambda_ode], lr=learning_rate)\n",
    "optimizer_ic = optim.Adam([lambda_ic], lr=learning_rate)\n",
    "optimizer_obs = optim.Adam([lambda_obs], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4e76b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : Total loss 26780444.0 , ode loss: 465478.596 , ode lambda: 1.022 , ic loss: 4166.177 , ic lambda: 1.025 , obs loss: 4166.177 , obs lambda: 1.017\n",
      "epoch 6 : Total loss 28051840.0 , ode loss: 663023.462 , ode lambda: 1.148 , ic loss: 4165.356 , ic lambda: 1.15 , obs loss: 4165.356 , obs lambda: 1.1\n",
      "epoch 11 : Total loss 30108844.0 , ode loss: 638582.414 , ode lambda: 1.271 , ic loss: 4158.489 , ic lambda: 1.275 , obs loss: 4158.489 , obs lambda: 1.182\n",
      "epoch 16 : Total loss 29300620.0 , ode loss: 1004266.722 , ode lambda: 1.407 , ic loss: 3861.496 , ic lambda: 1.397 , obs loss: 3861.496 , obs lambda: 1.262\n",
      "epoch 21 : Total loss 28456912.0 , ode loss: 1411752.369 , ode lambda: 1.597 , ic loss: 3625.68 , ic lambda: 1.51 , obs loss: 3625.68 , obs lambda: 1.332\n",
      "epoch 26 : Total loss 29857364.0 , ode loss: 1296845.062 , ode lambda: 1.757 , ic loss: 3626.538 , ic lambda: 1.626 , obs loss: 3626.538 , obs lambda: 1.403\n",
      "epoch 31 : Total loss 33608732.0 , ode loss: 656196.042 , ode lambda: 1.904 , ic loss: 3808.605 , ic lambda: 1.745 , obs loss: 3808.605 , obs lambda: 1.478\n",
      "epoch 36 : Total loss 32741516.0 , ode loss: 1161205.822 , ode lambda: 2.012 , ic loss: 3657.974 , ic lambda: 1.864 , obs loss: 3657.974 , obs lambda: 1.556\n",
      "epoch 41 : Total loss 33601536.0 , ode loss: 1115616.603 , ode lambda: 2.124 , ic loss: 3708.28 , ic lambda: 1.985 , obs loss: 3708.28 , obs lambda: 1.634\n",
      "epoch 46 : Total loss 35569944.0 , ode loss: 1129383.588 , ode lambda: 2.238 , ic loss: 3685.338 , ic lambda: 2.107 , obs loss: 3685.338 , obs lambda: 1.715\n",
      "epoch 51 : Total loss 37081784.0 , ode loss: 1121838.194 , ode lambda: 2.351 , ic loss: 3710.164 , ic lambda: 2.23 , obs loss: 3710.164 , obs lambda: 1.798\n",
      "epoch 56 : Total loss 38981228.0 , ode loss: 1128269.391 , ode lambda: 2.466 , ic loss: 3698.551 , ic lambda: 2.353 , obs loss: 3698.551 , obs lambda: 1.883\n",
      "epoch 61 : Total loss 41248220.0 , ode loss: 1140471.116 , ode lambda: 2.585 , ic loss: 3684.129 , ic lambda: 2.476 , obs loss: 3684.129 , obs lambda: 1.971\n",
      "epoch 66 : Total loss 42924532.0 , ode loss: 1118282.153 , ode lambda: 2.706 , ic loss: 3685.085 , ic lambda: 2.599 , obs loss: 3685.085 , obs lambda: 2.059\n",
      "epoch 71 : Total loss 45035704.0 , ode loss: 1185508.262 , ode lambda: 2.827 , ic loss: 3692.038 , ic lambda: 2.722 , obs loss: 3692.038 , obs lambda: 2.149\n",
      "epoch 76 : Total loss 46298200.0 , ode loss: 1110483.138 , ode lambda: 2.949 , ic loss: 3703.319 , ic lambda: 2.846 , obs loss: 3703.319 , obs lambda: 2.237\n",
      "epoch 81 : Total loss 49304128.0 , ode loss: 1122728.859 , ode lambda: 3.086 , ic loss: 3675.831 , ic lambda: 2.969 , obs loss: 3675.831 , obs lambda: 2.323\n",
      "epoch 86 : Total loss 49993256.0 , ode loss: 1140411.075 , ode lambda: 3.203 , ic loss: 3705.626 , ic lambda: 3.093 , obs loss: 3705.626 , obs lambda: 2.405\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m batch_loss_obs \u001b[38;5;241m=\u001b[39m loss_obs(net, t_tensor, SIR_obs)\n\u001b[0;32m     35\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m lambda_ode \u001b[38;5;241m*\u001b[39m batch_loss_ode \u001b[38;5;241m+\u001b[39m lambda_ic \u001b[38;5;241m*\u001b[39m batch_loss_ic \u001b[38;5;241m+\u001b[39m lambda_obs \u001b[38;5;241m*\u001b[39m batch_loss_obs \u001b[38;5;66;03m# with soft adaptation\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#maximize gradients of lambdas by inverting the gradient\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train network\n",
    "\n",
    "#getting epoch sizes\n",
    "num_samples_train = t_tensor.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "cur_loss = 0\n",
    "losses, ode_losses, ic_losses, obs_losses = [], [], [], []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size) #get slices for each batch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    ode_loss = 0\n",
    "    ic_loss = 0\n",
    "    obs_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        # Zero the gradients for all optimizers\n",
    "        optimizer_weights.zero_grad()\n",
    "        optimizer_ode.zero_grad()\n",
    "        optimizer_ic.zero_grad()\n",
    "        \n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(t_tensor[slce])\n",
    "\n",
    "        # compute gradients given loss\n",
    "        batch_loss_ode = loss_ode(net, t_tensor)\n",
    "        batch_loss_ic = loss_ic(net)\n",
    "        batch_loss_obs = loss_obs(net, t_tensor, SIR_obs)\n",
    "        batch_loss = lambda_ode * batch_loss_ode + lambda_ic * batch_loss_ic + lambda_obs * batch_loss_obs # with soft adaptation\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        #maximize gradients of lambdas by inverting the gradient\n",
    "        with torch.no_grad():\n",
    "            lambda_ode.grad *= -1\n",
    "            lambda_ic.grad *= -1\n",
    "            lambda_obs.grad *= -1\n",
    "        \n",
    "        #update net and lambdas\n",
    "        optimizer_weights.step()\n",
    "        optimizer_ode.step()\n",
    "        optimizer_ic.step()\n",
    "        optimizer_obs.step()\n",
    "\n",
    "        cur_loss += batch_loss.detach().numpy()\n",
    "        ode_loss += batch_loss_ode.detach().numpy()\n",
    "        ic_loss += batch_loss_ic.detach().numpy()\n",
    "        obs_loss += batch_loss_ic.detach().numpy()\n",
    "        \n",
    "    losses.append(cur_loss / batch_size)\n",
    "    ode_losses.append(ode_loss / batch_size)\n",
    "    ic_losses.append(ic_loss / batch_size)\n",
    "    obs_losses.append(ic_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch+1} : Total loss {np.round(losses[-1].item(), decimals=3)} , \"\n",
    "            f\"ode loss: {np.round(ode_losses[-1].item(), decimals=3)} , \"\n",
    "            f\"ode lambda: {np.round(lambda_ode.item(), decimals=3)} , \"\n",
    "            f\"ic loss: {np.round(ic_losses[-1].item(), decimals=3)} , \"\n",
    "            f\"ic lambda: {np.round(lambda_ic.item(), decimals=3)} , \"\n",
    "            f\"obs loss: {np.round(obs_losses[-1].item(), decimals=3)} , \"\n",
    "            f\"obs lambda: {np.round(lambda_obs.item(), decimals=3)}\"\n",
    "    )\n",
    "        \n",
    "        \n",
    "epoch = np.arange(len(losses))\n",
    "plt.figure()\n",
    "plt.plot(epoch, losses, 'r')\n",
    "plt.plot(epoch, ode_losses, 'g')\n",
    "plt.plot(epoch, ic_losses, 'b')\n",
    "plt.plot(epoch, obs_losses, 'c')\n",
    "plt.legend(['Loss', 'ODE loss', 'IC loss', \"Obs loss\"])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2014f5f",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64e895-396c-4ff1-92b2-86fab58abb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR_net_numpy(t):\n",
    "    SIR = net(t)\n",
    "    S_net = SIR[:, 0].detach().numpy()\n",
    "    I_net = SIR[:, 1].detach().numpy()\n",
    "    R_net = SIR[:, 2].detach().numpy()\n",
    "    return S_net, I_net, R_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c750ea-8ddc-4a6d-a8fa-e84d3df0f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical solution\n",
    "S_num, I_num, R_num = SIR_numerical(t_array)\n",
    "#PINN solution\n",
    "S_net, I_net, R_net = SIR_net_numpy(t_tensor)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t_array, S_num, color=\"blue\")\n",
    "plt.plot(t_array, I_num, color=\"red\")\n",
    "plt.plot(t_array, R_num, color=\"green\")\n",
    "plt.plot(t_array, S_net, color=\"blue\", linestyle=\"--\")\n",
    "plt.plot(t_array, I_net, color=\"red\", linestyle=\"--\")\n",
    "plt.plot(t_array, R_net, color=\"green\", linestyle=\"--\")\n",
    "plt.xlabel('t (in days)', fontsize = 12)\n",
    "plt.ylabel('I(t), S(t), R(t)', fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.grid(True)\n",
    "plt.legend([\"S (num)\", \"I (num)\", \"R (num)\", \"S (PINN)\", \"I (PINN)\", \"R ( PINN)\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
