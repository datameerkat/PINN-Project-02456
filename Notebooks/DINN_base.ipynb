{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b250a993",
   "metadata": {},
   "source": [
    "## PINN for predicting a SIR model\n",
    "This is the base case of modeling a SIR model using a Feed Forward Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9ebd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ac5fb-2d17-4360-aaf7-eefe9d754914",
   "metadata": {},
   "source": [
    "### Define time, initial conditions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ab05ff-211b-415c-88b5-d2cd39d2290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define amount of days being predicted and create time tensor\n",
    "t_days = 500\n",
    "t_tensor = torch.linspace(0, t_days, t_days+1, requires_grad = True).view(-1, 1)\n",
    "t_0 = torch.tensor([0]).view(-1,1)\n",
    "\n",
    "#define initial conditions for SIR model\n",
    "S_0 = 9999\n",
    "I_0 = 1\n",
    "R_0 = 0\n",
    "\n",
    "#define model parameters\n",
    "alpha = 1\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d14d397-5e6b-44fe-8e36-32fac5dc97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ode(net, t):\n",
    "    SIR = net(t)\n",
    "    print(SIR.size())\n",
    "    \n",
    "    # Calculate derivates\n",
    "    dSdt = torch.autograd.grad(SIR[:,0], t, grad_outputs=SIR[:,0], create_graph=True)[0]\n",
    "    dIdt = torch.autograd.grad(SIR[:,1], t, grad_outputs=SIR[:,1], create_graph=True)[0]\n",
    "    dRdt = torch.autograd.grad(SIR[:,2], t, grad_outputs=SIR[:,2], create_graph=True)[0]\n",
    "    print(dSdt.size())\n",
    "    \n",
    "    # Calculate the residuals using vectorized operations\n",
    "    S, I, R = SIR[:, 0], SIR[:, 1], SIR[:, 2]\n",
    "    S_r = dSdt - (- beta * S * I)\n",
    "    I_r = dIdt - (beta * S * I - alpha * I)\n",
    "    R_r = dRdt - (alpha * I)\n",
    "    \n",
    "    # Combine the residuals into a single tensor and calculate the mean squared error\n",
    "    residuals = torch.cat((S_r, I_r, R_r))\n",
    "    loss_ode = torch.mean(residuals**2)\n",
    "    return loss_ode\n",
    "\n",
    "\n",
    "#loss function for initial conditoons of S, I and R\n",
    "def loss_ic(net):\n",
    "    SIR_t0 = net(t_0)\n",
    "    loss_ic_squared = (SIR_t0[:, 0]-S_0)**2 + (SIR_t0[:, 1]-I_0)**2 + (SIR_t0[:, 2]-R_0)**2\n",
    "    loss_ic = torch.sqrt(loss_ic_squared)/3\n",
    "    return loss_ic\n",
    "\n",
    "def loss_obs(net, t, SIR_obs):\n",
    "    ## still has to be done##\n",
    "    return loss_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea83179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network architecture\n",
    "input_dim = 1\n",
    "output_dim = 3\n",
    "num_hidden = 50\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(num_hidden, num_hidden),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, output_dim),\n",
    "        )\n",
    "        \n",
    "        # Apply Kaiming initialization to the layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, t):\n",
    "        SIR = self.linear_relu_stack(t)\n",
    "        return SIR\n",
    "    \n",
    "net = Net(num_hidden)\n",
    "    \n",
    "# hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 20\n",
    "num_epochs = 50\n",
    "\n",
    "#initialize lambdas for soft-adaptation\n",
    "lambda_ode = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "lambda_ic = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "lambda_obs = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "\n",
    "#optimizer: weights updates the net, ode and ic update the lambda for soft-adaptation\n",
    "optimizer_weights = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer_ode = optim.Adam([lambda_ode], lr=learning_rate)\n",
    "optimizer_ic = optim.Adam([lambda_ic], lr=learning_rate)\n",
    "optimizer_obs = optim.Adam([lambda_obs], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4e76b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([501, 3])\n",
      "torch.Size([501, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# compute gradients given loss\u001b[39;00m\n\u001b[0;32m     32\u001b[0m batch_loss_ode \u001b[38;5;241m=\u001b[39m loss_ode(net, t_tensor)\n\u001b[1;32m---> 33\u001b[0m batch_loss_ic \u001b[38;5;241m=\u001b[39m \u001b[43mloss_ic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m lambda_ode \u001b[38;5;241m*\u001b[39m batch_loss_ode \u001b[38;5;241m+\u001b[39m lambda_ic \u001b[38;5;241m*\u001b[39m batch_loss_ic \u001b[38;5;241m+\u001b[39m lambda_obs \u001b[38;5;241m*\u001b[39m batch_loss_obs \u001b[38;5;66;03m# with soft adaptation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mloss_ic\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_ic\u001b[39m(net):\n\u001b[1;32m---> 25\u001b[0m     SIR_t0 \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     loss_ic_squared \u001b[38;5;241m=\u001b[39m (SIR_t0[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39mS_0)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (SIR_t0[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mI_0)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (SIR_t0[:, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m-\u001b[39mR_0)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     27\u001b[0m     loss_ic \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(loss_ic_squared)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[1;32m---> 29\u001b[0m     SIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SIR\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Long"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "\n",
    "#getting epoch sizes\n",
    "num_samples_train = t_tensor.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "cur_loss = 0\n",
    "losses, ode_losses, ic_losses, obs_losses = [], [], [], []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size) #get slices for each batch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    ode_loss = 0\n",
    "    ic_loss = 0\n",
    "    obs_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        # Zero the gradients for all optimizers\n",
    "        optimizer_weights.zero_grad()\n",
    "        optimizer_ode.zero_grad()\n",
    "        optimizer_ic.zero_grad()\n",
    "        \n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(t_tensor[slce])\n",
    "\n",
    "        # compute gradients given loss\n",
    "        batch_loss_ode = loss_ode(net, t_tensor)\n",
    "        batch_loss_ic = loss_ic(net)\n",
    "        batch_loss = lambda_ode * batch_loss_ode + lambda_ic * batch_loss_ic + lambda_obs * batch_loss_obs # with soft adaptation\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        #maximize gradients of lambdas by inverting the gradient\n",
    "        with torch.no_grad():\n",
    "            lambda_ode.grad *= -1\n",
    "            lambda_ic.grad *= -1\n",
    "            lambda_obs.grad *= -1\n",
    "        \n",
    "        #update net and lambdas\n",
    "        optimizer_weights.step()\n",
    "        optimizer_ode.step()\n",
    "        optimizer_ic.step()\n",
    "        optimizer_obs.step()\n",
    "\n",
    "        cur_loss += batch_loss.detach().numpy()\n",
    "        ode_loss += batch_loss_ode.detach().numpy()\n",
    "        ic_loss += batch_loss_ic.detach().numpy()\n",
    "        obs_loss += batch_loss_ic.detach().numpy()\n",
    "        \n",
    "    losses.append(cur_loss / batch_size)\n",
    "    ode_losses.append(ode_loss / batch_size)\n",
    "    ic_losses.append(ic_loss / batch_size)\n",
    "    obs_losses.append(ic_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch+1} : Total loss {np.round(losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ode loss: {np.round(ode_losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ode lambda: {np.round(lambda_ode.item(), decimals=3)} , \"\n",
    "            f\"ic loss: {np.round(ic_losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ic lambda: {np.round(lambda_ic.item(), decimals=3)}\"\n",
    "    )\n",
    "        \n",
    "        \n",
    "epoch = np.arange(len(losses))\n",
    "plt.figure()\n",
    "plt.plot(epoch, losses, 'r')\n",
    "plt.plot(epoch, ode_losses, 'g')\n",
    "plt.plot(epoch, ic_losses, 'b')\n",
    "plt.legend(['Loss', 'ODE loss', 'IC loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2014f5f",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c750ea-8ddc-4a6d-a8fa-e84d3df0f6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
