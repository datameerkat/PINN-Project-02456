{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b250a993",
   "metadata": {},
   "source": [
    "## PINN for predicting a SIR model\n",
    "This is the base case of modeling a SIR model using a Feed Forward Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9ebd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ac5fb-2d17-4360-aaf7-eefe9d754914",
   "metadata": {},
   "source": [
    "### Define time, initial conditions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ab05ff-211b-415c-88b5-d2cd39d2290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define amount of days being predicted and create time tensor\n",
    "t_days = 500\n",
    "t_tensor = torch.linspace(0, t_days, t_days+1, requires_grad = True).view(-1, 1)\n",
    "t_0 = torch.tensor([0], dtype=torch.float32).view(-1,1)\n",
    "\n",
    "#define initial conditions for SIR model\n",
    "S_0 = 9999\n",
    "I_0 = 1\n",
    "R_0 = 0\n",
    "\n",
    "#define model parameters\n",
    "alpha = 1\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d14d397-5e6b-44fe-8e36-32fac5dc97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ode(net, t):\n",
    "    SIR = net(t)\n",
    "    print(SIR.size())\n",
    "    \n",
    "    # Calculate derivates\n",
    "    dSdt = torch.autograd.grad(SIR[:,0], t, grad_outputs=SIR[:,0], create_graph=True)[0]\n",
    "    dIdt = torch.autograd.grad(SIR[:,1], t, grad_outputs=SIR[:,1], create_graph=True)[0]\n",
    "    dRdt = torch.autograd.grad(SIR[:,2], t, grad_outputs=SIR[:,2], create_graph=True)[0]\n",
    "    print(dSdt.size())\n",
    "    \n",
    "    # Calculate the residuals using vectorized operations\n",
    "    S, I, R = SIR[:, 0], SIR[:, 1], SIR[:, 2]\n",
    "    S_r = dSdt - (- beta * S * I)\n",
    "    I_r = dIdt - (beta * S * I - alpha * I)\n",
    "    R_r = dRdt - (alpha * I)\n",
    "    \n",
    "    # Combine the residuals into a single tensor and calculate the mean squared error\n",
    "    residuals = torch.cat((S_r, I_r, R_r))\n",
    "    loss_ode = torch.mean(residuals**2)\n",
    "    return loss_ode\n",
    "\n",
    "\n",
    "#loss function for initial conditoons of S, I and R\n",
    "def loss_ic(net):\n",
    "    SIR_t0 = net(t_0)\n",
    "    loss_ic_squared = (SIR_t0[:, 0]-S_0)**2 + (SIR_t0[:, 1]-I_0)**2 + (SIR_t0[:, 2]-R_0)**2\n",
    "    loss_ic = torch.sqrt(loss_ic_squared)/3\n",
    "    return loss_ic\n",
    "\n",
    "def loss_obs(net, t, SIR_obs):\n",
    "    ## still has to be done##\n",
    "    return loss_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea83179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network architecture\n",
    "input_dim = 1\n",
    "output_dim = 3\n",
    "num_hidden = 50\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(num_hidden, num_hidden),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, output_dim),\n",
    "        )\n",
    "        \n",
    "        # Apply Kaiming initialization to the layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, t):\n",
    "        SIR = self.linear_relu_stack(t)\n",
    "        return SIR\n",
    "    \n",
    "net = Net(num_hidden)\n",
    "    \n",
    "# hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 20\n",
    "num_epochs = 50\n",
    "\n",
    "#initialize lambdas for soft-adaptation\n",
    "lambda_ode = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "lambda_ic = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "lambda_obs = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "\n",
    "#optimizer: weights updates the net, ode and ic update the lambda for soft-adaptation\n",
    "optimizer_weights = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer_ode = optim.Adam([lambda_ode], lr=learning_rate)\n",
    "optimizer_ic = optim.Adam([lambda_ic], lr=learning_rate)\n",
    "optimizer_obs = optim.Adam([lambda_obs], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4e76b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([501, 3])\n",
      "torch.Size([501, 1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_loss_obs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m batch_loss_ode \u001b[38;5;241m=\u001b[39m loss_ode(net, t_tensor)\n\u001b[0;32m     33\u001b[0m batch_loss_ic \u001b[38;5;241m=\u001b[39m loss_ic(net)\n\u001b[1;32m---> 34\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m lambda_ode \u001b[38;5;241m*\u001b[39m batch_loss_ode \u001b[38;5;241m+\u001b[39m lambda_ic \u001b[38;5;241m*\u001b[39m batch_loss_ic \u001b[38;5;241m+\u001b[39m lambda_obs \u001b[38;5;241m*\u001b[39m \u001b[43mbatch_loss_obs\u001b[49m \u001b[38;5;66;03m# with soft adaptation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#maximize gradients of lambdas by inverting the gradient\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_loss_obs' is not defined"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "\n",
    "#getting epoch sizes\n",
    "num_samples_train = t_tensor.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "cur_loss = 0\n",
    "losses, ode_losses, ic_losses, obs_losses = [], [], [], []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size) #get slices for each batch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    ode_loss = 0\n",
    "    ic_loss = 0\n",
    "    obs_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        # Zero the gradients for all optimizers\n",
    "        optimizer_weights.zero_grad()\n",
    "        optimizer_ode.zero_grad()\n",
    "        optimizer_ic.zero_grad()\n",
    "        \n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(t_tensor[slce])\n",
    "\n",
    "        # compute gradients given loss\n",
    "        batch_loss_ode = loss_ode(net, t_tensor)\n",
    "        batch_loss_ic = loss_ic(net)\n",
    "        batch_loss = lambda_ode * batch_loss_ode + lambda_ic * batch_loss_ic + lambda_obs * batch_loss_obs # with soft adaptation\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        #maximize gradients of lambdas by inverting the gradient\n",
    "        with torch.no_grad():\n",
    "            lambda_ode.grad *= -1\n",
    "            lambda_ic.grad *= -1\n",
    "            lambda_obs.grad *= -1\n",
    "        \n",
    "        #update net and lambdas\n",
    "        optimizer_weights.step()\n",
    "        optimizer_ode.step()\n",
    "        optimizer_ic.step()\n",
    "        optimizer_obs.step()\n",
    "\n",
    "        cur_loss += batch_loss.detach().numpy()\n",
    "        ode_loss += batch_loss_ode.detach().numpy()\n",
    "        ic_loss += batch_loss_ic.detach().numpy()\n",
    "        obs_loss += batch_loss_ic.detach().numpy()\n",
    "        \n",
    "    losses.append(cur_loss / batch_size)\n",
    "    ode_losses.append(ode_loss / batch_size)\n",
    "    ic_losses.append(ic_loss / batch_size)\n",
    "    obs_losses.append(ic_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch+1} : Total loss {np.round(losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ode loss: {np.round(ode_losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ode lambda: {np.round(lambda_ode.item(), decimals=3)} , \"\n",
    "            f\"ic loss: {np.round(ic_losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ic lambda: {np.round(lambda_ic.item(), decimals=3)}\"\n",
    "    )\n",
    "        \n",
    "        \n",
    "epoch = np.arange(len(losses))\n",
    "plt.figure()\n",
    "plt.plot(epoch, losses, 'r')\n",
    "plt.plot(epoch, ode_losses, 'g')\n",
    "plt.plot(epoch, ic_losses, 'b')\n",
    "plt.legend(['Loss', 'ODE loss', 'IC loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2014f5f",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c750ea-8ddc-4a6d-a8fa-e84d3df0f6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
