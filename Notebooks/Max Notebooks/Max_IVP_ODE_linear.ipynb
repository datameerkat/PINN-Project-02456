{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b250a993",
   "metadata": {},
   "source": [
    "## Neural network for solving an inital value problem\n",
    "SA-PINN example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbfde0-eade-4448-9a0b-1d7c33fc21fa",
   "metadata": {},
   "source": [
    "Things to change\n",
    "- Exact solution: u0 * exp(h*t)\n",
    "- Compare net against real solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9ebd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc52e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ab05ff-211b-415c-88b5-d2cd39d2290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define domain\n",
    "t_min, t_max = [0, 2]\n",
    "h_min, h_max = [-1, 1]\n",
    "t_steps, h_steps = [int(50000), int(10)]\n",
    "u0 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e5c509-bd80-4cc7-bc16-2b1122c32b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500000, 1])\n",
      "torch.Size([500000, 1])\n"
     ]
    }
   ],
   "source": [
    "#creates the input vector with the dimensions of (t_steps*h_steps, 2), where t values are in the first and h values in the second column\n",
    "th_np = np.zeros((t_steps*h_steps,2))\n",
    "t_np = np.linspace(t_min, t_max, num=t_steps)\n",
    "h_np = np.linspace(h_min, h_max, num=h_steps)\n",
    "for i, h in enumerate(h_np):\n",
    "    th_np[i*t_steps:(i+1)*t_steps,0] = t_np\n",
    "    th_np[i*t_steps:(i+1)*t_steps,1] = np.full((t_steps), h)\n",
    "    \n",
    "#randomize input\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "th_np_rand = th_np\n",
    "rng.shuffle(th_np_rand, axis=0)\n",
    "t_np_rand = th_np_rand[:,0]\n",
    "h_np_rand = th_np_rand[:,1]\n",
    "\n",
    "#create tensors from numpy arrays > change here if you want an organized or random input!!!!\n",
    "t_tensor = torch.tensor(t_np_rand, requires_grad=True).float().view(-1, 1)\n",
    "h_tensor = torch.tensor(h_np_rand, requires_grad=True).float().view(-1, 1)\n",
    "print(t_tensor.shape)\n",
    "print(h_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284fc169-85a1-4083-bd91-27c47e109c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss functions\n",
    "# loss_ode: (h*u_net) - u_net'\n",
    "# loss_ic: u_net(t0) - u0\n",
    "def loss_ode(net, t, h):\n",
    "    u = net(t, h)\n",
    "\n",
    "    #First derivative\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    F = u*h\n",
    "    loss_ode = torch.mean((F-u_t)**2)\n",
    "    \n",
    "    return loss_ode\n",
    "def loss_ic(net, t, h):\n",
    "    #set x[:,0] to 0, leave x[:,1] as it is\n",
    "    #just substract u0 from it \n",
    "    t_0 = torch.ones(t_tensor.shape)\n",
    "    u_t0 = net(t0, h)\n",
    "    \n",
    "    loss_ic = torch.mean((u_t0-u0)**2)\n",
    "    \n",
    "    return loss_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea83179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network architecture\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "num_hidden = 300\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(num_hidden, output_dim),\n",
    "        )\n",
    "        \n",
    "        # Apply Kaiming initialization to the layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, t, h):\n",
    "        x = torch.cat((t, h), 1)\n",
    "        y_out = self.linear_relu_stack(x)\n",
    "        return y_out\n",
    "    \n",
    "net = Net(num_hidden)\n",
    "    \n",
    "# hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 20\n",
    "num_epochs = 500\n",
    "\n",
    "#initialize lambdas for soft-adaptation\n",
    "lambda_ode = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "lambda_ic = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "\n",
    "#optimizer: weights updates the net, ode and ic update the lambda for soft-adaptation\n",
    "optimizer_weights = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer_ode = optim.Adam([lambda_ode], lr=learning_rate)\n",
    "optimizer_ic = optim.Adam([lambda_ic], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4e76b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m net(t_tensor[slce, :], h_tensor[slce, :])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# compute gradients given loss\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m batch_loss_ode \u001b[38;5;241m=\u001b[39m \u001b[43mloss_ode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m batch_loss_ic \u001b[38;5;241m=\u001b[39m loss_ic(net, t_tensor, h_tensor)\n\u001b[0;32m     35\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m lambda_ode \u001b[38;5;241m*\u001b[39m batch_loss_ode \u001b[38;5;241m+\u001b[39m lambda_ic \u001b[38;5;241m*\u001b[39m batch_loss_ic \u001b[38;5;66;03m# with soft adaptation\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mloss_ode\u001b[1;34m(net, t, h)\u001b[0m\n\u001b[0;32m      5\u001b[0m u \u001b[38;5;241m=\u001b[39m net(t, h)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#First derivative\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m u_t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m F \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m*\u001b[39mh\n\u001b[0;32m     11\u001b[0m loss_ode \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((F\u001b[38;5;241m-\u001b[39mu_t)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:234\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train network\n",
    "\n",
    "#getting epoch sizes\n",
    "num_samples_train = t_tensor.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "#test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses, ode_losses, ic_losses = [], [], []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size) #get slices for each batch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    ode_loss = 0\n",
    "    ic_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        # Zero the gradients for all optimizers\n",
    "        optimizer_weights.zero_grad()\n",
    "        optimizer_ode.zero_grad()\n",
    "        optimizer_ic.zero_grad()\n",
    "        \n",
    "        slce = get_slice(i, batch_size)\n",
    "        #x_slice = th_tensor[slce, :]\n",
    "        output = net(t_tensor[slce, :], h_tensor[slce, :])\n",
    "\n",
    "        # compute gradients given loss\n",
    "        batch_loss_ode = loss_ode(net, t_tensor, h_tensor)\n",
    "        batch_loss_ic = loss_ic(net, t_tensor, h_tensor)\n",
    "        batch_loss = lambda_ode * batch_loss_ode + lambda_ic * batch_loss_ic # with soft adaptation\n",
    "        batch_loss.backward()\n",
    "        #check if pde_loss has gradient\n",
    "        assert any(param.grad is not None for param in net.parameters()) == True\n",
    "        \n",
    "        #maximize gradients of lambdas by inverting the gradient\n",
    "        with torch.no_grad():\n",
    "            lambda_ode.grad *= -1\n",
    "            lambda_ic.grad *= -1\n",
    "        \n",
    "        #update net and lambdas\n",
    "        optimizer_weights.step()\n",
    "        optimizer_ode.step()\n",
    "        optimizer_ic.step()\n",
    "\n",
    "        cur_loss += batch_loss.detach().numpy()\n",
    "        ode_loss += batch_loss_ode.detach().numpy()\n",
    "        ic_loss += batch_loss_ic.detach().numpy()\n",
    "        \n",
    "    losses.append(cur_loss / batch_size)\n",
    "    ode_losses.append(ode_loss / batch_size)\n",
    "    ic_losses.append(ic_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch+1} : Total loss {np.round(losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ode loss: {np.round(ode_losses[-1].item(), decimals=6)} , \"\n",
    "            f\"pde lambda: {np.round(lambda_ode.item(), decimals=3)} , \"\n",
    "            f\"ic loss: {np.round(ic_losses[-1].item(), decimals=6)} , \"\n",
    "            f\"ic lambda: {np.round(lambda_ic.item(), decimals=3)}\"\n",
    "    )\n",
    "        \n",
    "        \n",
    "epoch = np.arange(len(losses))\n",
    "plt.figure()\n",
    "plt.plot(epoch, losses, 'r')\n",
    "plt.plot(epoch, ode_losses, 'g')\n",
    "plt.plot(epoch, ic_losses, 'b')\n",
    "plt.legend(['Loss', 'ODE loss', 'IC loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2014f5f",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8d76d-9610-4d20-bc5a-3cfd0cd2f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define analytical solution\n",
    "def analytical_solution(t, h, u0):\n",
    "    return u0 * np.exp(h * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aeb7ab-c0b4-4641-8165-f3298269e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define domain for plotting the net output\n",
    "t_values = np.linspace(t_min, t_max, t_steps)\n",
    "h_values = np.linspace(h_min, h_max, h_steps)\n",
    "\n",
    "# Convert t_values to a tensor for input to the neural network\n",
    "t_tensor_plot = torch.tensor(t_values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define a color palette\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(h_values)))\n",
    "\n",
    "# Plot the analytical solutions\n",
    "for i, h in enumerate(h_values):\n",
    "    u_analytical = analytical_solution(t_values, h, u0)\n",
    "    plt.plot(t_values, u_analytical, label=f'h={h:.1f}',color=colors[i])\n",
    "    # Label the curves\n",
    "    #plt.text(t_values[-1], u_analytical[-1], f'h={h:.1f}')\n",
    "\n",
    "# Plot the neural network predictions\n",
    "for i, h in enumerate(h_values):\n",
    "    h_tensor_plot = torch.full((t_steps,), h, dtype=torch.float32).view(-1, 1)  # tensor full of h\n",
    "    u_nn = net(t_tensor_plot, h_tensor_plot).detach().numpy()  # neural network prediction\n",
    "    plt.plot(t_values, u_nn, color=colors[i], linestyle=\"dotted\")\n",
    "\n",
    "plt.title('Comparison of Analytical Solution and Neural Network Prediction')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('u')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c750ea-8ddc-4a6d-a8fa-e84d3df0f6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
